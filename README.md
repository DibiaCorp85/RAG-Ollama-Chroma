ğŸ“š RAG-Ollama-Chroma
A modern Retrieval-Augmented Generation (RAG) system powered by ChromaDB, Ollama LLM, and LangChain for efficient document retrieval and AI-generated responses. Supports multi-format document ingestion (.txt, .pdf, .docx) and provides two retrieval interfaces:

âœ… Gradio Web UI â€“ for easy question-answering
âœ… Chainlit Chatbot â€“ for interactive, streaming responses with multi-turn memory

ğŸš€ Features
âœ… Multi-format document ingestion (.txt, .pdf, .docx)
âœ… Fast document embeddings with HuggingFace
âœ… Efficient vector search using ChromaDB
âœ… AI-powered retrieval using Ollama's wizardlm2 model
âœ… Web-based question-answering interface with Gradio
âœ… Interactive chatbot with streaming responses via Chainlit
âœ… Multi-turn memory support (Chainlit)


ğŸ› ï¸ Installation & Setup
1ï¸âƒ£ Clone the Repository

git clone https://github.com/YOUR_GITHUB_USERNAME/RAG-Ollama-Chroma.git
cd RAG-Ollama-Chroma

2ï¸âƒ£ Set Up a Python Virtual Environment
  
venv\Scripts\activate  # Windows  
3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

4ï¸âƒ£ Start Ollama (if not running)
If you haven't installed Ollama, follow the installation guide and start the local server:

ollama serve

ğŸ“¥ Document Ingestion
To process and index documents in ChromaDB, run:

python document_ingestion.py
Supported formats: TXT, PDF, DOCX
This will store vector embeddings for retrieval.

ğŸ” Retrieval Interfaces
1ï¸âƒ£ Gradio Web UI
Run the Gradio interface for document retrieval:

python retrieval_gradio.py
This starts a web-based question-answering UI at:

2ï¸âƒ£ Chainlit Chatbot (Streaming)
Initialize chainlit, run "chainlit init" on the terminal (for windows)
To enable conversational AI retrieval, run:

chainlit run retrieval_chainlit.py
This launches a chatbot with real-time streaming responses.

ğŸ“„ Example Usage
1ï¸âƒ£ Run document_ingestion.py to index files.
2ï¸âƒ£ Start Gradio (retrieval_gradio.py) or Chainlit (retrieval_chainlit.py).
3ï¸âƒ£ Ask a question in Gradio or Chainlit:

"What are separation processes?"
4ï¸âƒ£ Get a context-aware AI response.

âš¡ TODO (Upcoming Features)
 Add hybrid search (keyword + semantic)
 Implement RAG caching for faster responses
 Enhance multi-turn chat memory
 Deploy as a FastAPI or Flask app

ğŸ‘¥ Contributing
Contributions are welcome!

Fork the repo
Create a feature branch (git checkout -b feature-name)
Commit changes (git commit -m "Add feature")
Push and create a Pull Request (PR)
ğŸ“œ License
This project is licensed under the MIT License. See the LICENSE file for details.

ğŸ™ Acknowledgements
ğŸ’¡ Built using:

LangChain â€“ For document processing
ChromaDB â€“ Vector storage
Ollama â€“ Local LLM for response generation
Gradio â€“ Web UI for retrieval
Chainlit â€“ Chatbot UI with streaming

ğŸ“© Questions or Issues?
Open an issue on GitHub or reach out for support. ğŸš€
Happy coding! ğŸ¯